import streamlit as st
from neo4j import GraphDatabase
from openai import OpenAI
from dotenv import load_dotenv
import os
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    roc_auc_score, roc_curve, precision_recall_curve,
    average_precision_score, classification_report, 
    precision_score, recall_score, f1_score, confusion_matrix
)
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
from collections import Counter
import uuid
import numpy as np
import time
import matplotlib.pyplot as plt
import plotly.express as px
from contextlib import contextmanager
import logging
from typing import Optional, Tuple
from sklearn.preprocessing import StandardScaler
from node2vec import Node2Vec
import networkx as nx
import tempfile
import shutil
import seaborn as sns
from sklearn.inspection import permutation_importance

# === Configuration ===
class Config:
    EMBEDDING_DIM = 64
    RANDOM_STATE = 42
    TEST_SIZE = 0.3  # Increased for better validation
    N_ESTIMATORS = 100
    SMOTE_RATIO = 'auto'
    MIN_CASES_FOR_ANOMALY_DETECTION = 10
    NODE2VEC_WALK_LENGTH = 20
    NODE2VEC_NUM_WALKS = 50
    NODE2VEC_WORKERS = 1
    NODE2VEC_P = 1
    NODE2VEC_Q = 1
    EMBEDDING_BATCH_SIZE = 50
    MAX_RELATIONSHIPS = 100000
    EMBEDDING_GENERATION_TIMEOUT = 300
    LEAKAGE_CHECK = True  # Enable rigorous leakage checks

# === Logging Setup ===
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# === Environment Setup ===
load_dotenv()

# Validate Environment Variables
required_env_vars = ["NEO4J_URI", "NEO4J_USER", "NEO4J_PASSWORD", "OPENAI_API_KEY"]
missing_vars = [var for var in required_env_vars if not os.getenv(var)]
if missing_vars:
    st.error(f"Missing required environment variables: {', '.join(missing_vars)}")
    st.stop()
# âœ… Load variables for use
NEO4J_URI = os.getenv("NEO4J_URI")
NEO4J_USER = os.getenv("NEO4J_USER")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# === Neo4j Service Class ===
class Neo4jService:
    def __init__(self, uri: str, user: str, password: str):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))

    @contextmanager
    def session(self):
        with self.driver.session() as session:
            yield session

    def close(self):
        self.driver.close()

# === Initialize Services ===
@st.cache_resource
def get_neo4j_service():
    return Neo4jService(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)

@st.cache_resource
def get_openai_client():
    return OpenAI(api_key=OPENAI_API_KEY)

neo4j_service = get_neo4j_service()
client = get_openai_client()

# === Helper Functions ===
def safe_neo4j_operation(func):
    """Decorator for Neo4j operations with error handling"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            st.error(f"Neo4j operation failed: {str(e)}")
            logger.error(f"Neo4j operation failed: {str(e)}")
            return None
    return wrapper

# === Data Insertion ===
@safe_neo4j_operation
def insert_user_case(row: pd.Series, upload_id: str) -> str:
    queries = []

    # âœ… Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î® ÎµÎ½Î·Î¼Î­ÏÏ‰ÏƒÎ· ÎºÏŒÎ¼Î²Î¿Ï… Case Î¼Îµ Î¼Î¿Î½Î±Î´Î¹ÎºÏŒ upload_id
    queries.append((
        "MERGE (c:Case {upload_id: $upload_id}) SET c.id = $id, c.embedding = null",
        {"upload_id": upload_id, "id": int(row["Case_No"])}
    ))

    # âœ… Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÏƒÏ‡Î­ÏƒÎµÏ‰Î½ Î¼Îµ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ ÏƒÏ…Î¼Ï€ÎµÏÎ¹Ï†Î¿ÏÎ¬Ï‚
    for i in range(1, 11):
        q = f"A{i}"
        val = int(row[q])
        queries.append((
            """
            MATCH (q:BehaviorQuestion {name: $q})
            MATCH (c:Case {upload_id: $upload_id})
            CREATE (c)-[:HAS_ANSWER {value: $val}]->(q)
            """,
            {"q": q, "val": val, "upload_id": upload_id}
        ))

    # âœ… Î”Î·Î¼Î¿Î³ÏÎ±Ï†Î¹ÎºÎ¬ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬
    demo = {
        "Sex": row["Sex"],
        "Ethnicity": row["Ethnicity"],
        "Jaundice": row["Jaundice"],
        "Family_mem_with_ASD": row["Family_mem_with_ASD"]
    }
    for k, v in demo.items():
        queries.append((
            """
            MATCH (d:DemographicAttribute {type: $k, value: $v})
            MATCH (c:Case {upload_id: $upload_id})
            CREATE (c)-[:HAS_DEMOGRAPHIC]->(d)
            """,
            {"k": k, "v": v, "upload_id": upload_id}
        ))

    # âœ… Î Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î± ÏƒÏ…Î¼Ï€Î»Î·ÏÏ‰Ï„Î®
    queries.append((
        """
        MATCH (s:SubmitterType {type: $who})
        MATCH (c:Case {upload_id: $upload_id})
        CREATE (c)-[:SUBMITTED_BY]->(s)
        """,
        {"who": row["Who_completed_the_test"], "upload_id": upload_id}
    ))

    # âœ… Î•ÎºÏ„Î­Î»ÎµÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ ÎµÏÏ‰Ï„Î·Î¼Î¬Ï„Ï‰Î½
    with neo4j_service.session() as session:
        for query, params in queries:
            session.run(query, **params)
        logger.info(f"âœ… Inserted new case with upload_id {upload_id}")

    return upload_id

@safe_neo4j_operation
def remove_screened_for_labels():
    with neo4j_service.session() as session:
        session.run("""
            MATCH (c:Case)-[r:SCREENED_FOR]->(:ASD_Trait)
            DELETE r
        """)
        logger.info("âœ… SCREENED_FOR relationships removed to prevent leakage.")


# === Graph Embeddings Generation ===
@safe_neo4j_operation
def generate_embedding_for_case(upload_id: str) -> bool:
    import subprocess, sys, os

    builder_path = os.path.join(os.path.dirname(__file__), "generate_case_embedding.py")
    if not os.path.exists(builder_path):
        st.error(f"âŒ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿: {builder_path}")
        return False

    try:
        proc = subprocess.run(
            [sys.executable, builder_path, upload_id],
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True
        )
        st.text(proc.stdout)

        if proc.returncode != 0:
            st.error("âŒ Î— Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± embedding Î³Î¹Î± Ï„Î¿ case Î±Ï€Î­Ï„Ï…Ï‡Îµ")
            return False
        return True
    except Exception as e:
        st.error(f"âŒ Î£Ï†Î¬Î»Î¼Î±: {e}")
        return False

    try:
        st.info(f"ğŸ“„ Î•ÎºÎºÎ¯Î½Î·ÏƒÎ·: `{builder_path}`")
        print(f"[DEBUG] Running kg_builder_2.py at: {builder_path}")
        proc = subprocess.Popen(
            [sys.executable, builder_path],
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True
        )

        output_lines = []
        for line in proc.stdout:
            output_lines.append(line)
            status_text.text(line.strip())
            st.text(line.strip())  # ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ· ÏƒÎµ Streamlit
            if not hasattr(progress_bar, "_current_progress"):
                progress_bar._current_progress = 10
            else:
                 progress_bar._current_progress = min(progress_bar._current_progress + 5, 95)

            progress_bar.progress(progress_bar._current_progress)

        proc.wait()
        if proc.returncode != 0:
            status_text.error("âŒ ÎŸ builder Î±Ï€Î­Ï„Ï…Ï‡Îµ (return code != 0)")
            st.code("".join(output_lines), language="bash")
            return False
        if proc.returncode != 0:
            #Î•ÎºÏ„ÏÏ€Ï‰ÏƒÎµ Ï„Î¿ Ï€Î»Î®ÏÎµÏ‚ output Î±Î½ Î±Ï€Î­Ï„Ï…Ï‡Îµ
            st.error("âŒ ÎŸ builder Î±Ï€Î­Ï„Ï…Ï‡Îµ (return code != 0)")
            st.code("".join(output_lines), language="bash")  # âœ… Î ÏÎ¿ÏƒÎ¸Î­Ï„ÎµÎ¹ ÎµÎ¼Ï†Î±Î½Î­Ï‚ log
            return False

        progress_bar.progress(100)
        status_text.text("âœ… Embeddings generated and stored!")
        return True

    except Exception as e:
        status_text.error(f"âŒ Î£Ï†Î¬Î»Î¼Î±: {e}")
        return False

    finally:
        time.sleep(1)
        status_text.empty()
        progress_bar.empty()
# === Natural Language to Cypher ===
def nl_to_cypher(question: str) -> Optional[str]:
    """Translates natural language to Cypher using OpenAI"""
    prompt = f"""
    You are a Cypher expert working with a Neo4j Knowledge Graph about toddlers and autism.

    Schema:
    - (:Case {{id: int}})
    - (:BehaviorQuestion {{name: string}})
    - (:ASD_Trait {{value: 'Yes' | 'No'}})
    - (:DemographicAttribute {{type: 'Sex' | 'Ethnicity' | 'Jaundice' | 'Family_mem_with_ASD', value: string}})
    - (:SubmitterType {{type: string}})

    Relationships:
    - (:Case)-[:HAS_ANSWER {{value: int}}]->(:BehaviorQuestion)
    - (:Case)-[:HAS_DEMOGRAPHIC]->(:DemographicAttribute)
    - (:Case)-[:SCREENED_FOR]->(:ASD_Trait)
    - (:Case)-[:SUBMITTED_BY]->(:SubmitterType)

    Rules:
    1. Always use `toLower()` for case-insensitive comparisons
    2. Interpret 'f' as 'female' and 'm' as 'male' for Sex
    3. Never use SCREENED_FOR relationships in training queries

    Translate this question to Cypher:
    Q: {question}

    Return ONLY the Cypher query.
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        cypher_query = response.choices[0].message.content.strip()
        return cypher_query.replace("```cypher", "").replace("```", "").strip()
    except Exception as e:
        st.error(f"OpenAI API error: {e}")
        logger.error(f"OpenAI API error: {e}")
        return None

# === Embedding Extraction ===
@safe_neo4j_operation
def extract_user_embedding(upload_id: str) -> Optional[np.ndarray]:
    """Safely extracts embedding for a specific case"""
    with neo4j_service.session() as session:
        result = session.run(
            "MATCH (c:Case {upload_id: $upload_id}) RETURN c.embedding AS embedding",
            upload_id=upload_id
        )
        record = result.single()
        
        if record and record["embedding"] is not None:
            return np.array(record["embedding"]).reshape(1, -1)
        
        # Verify if case exists
        exists = session.run(
            "MATCH (c:Case {upload_id: $upload_id}) RETURN count(c) > 0 AS exists",
            upload_id=upload_id
        ).single()["exists"]
        
        if not exists:
            st.error(f"âŒ Case with upload_id {upload_id} not found")
        else:
            st.error(f"âŒ No embedding found for case {upload_id}. Please regenerate embeddings.")
        
        return None

from sklearn.impute import SimpleImputer

# === Training Data Preparation ===
@safe_neo4j_operation
def extract_training_data_from_csv(file_path: str) -> Tuple[pd.DataFrame, pd.Series]:
    """Extracts training data with leakage protection and NaN handling"""
    try:
        df = pd.read_csv(file_path, delimiter=";", encoding='utf-8-sig')

        # ğŸ”§ ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ„Î·Î»ÏÎ½ (Î±Ï†Î±Î¹ÏÎµÎ¯ ÎºÎ±Î¹ ÎºÏÏ…Ï†Î¬ ÎºÎ±Î¹ Ï€ÎµÏÎ¹Ï„Ï„Î¬ ÎºÎµÎ½Î¬)
        df.columns = [col.strip().replace('\r', '') for col in df.columns]
        df.columns = [col.strip() for col in df.columns]  # Ï„ÎµÎ»Î¹ÎºÎ® Î¼Î¿ÏÏ†Î®

        # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± Î²Î±ÏƒÎ¹ÎºÎ­Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚
        required_cols = ["Case_No", "Class_ASD_Traits"]
        missing = [col for col in required_cols if col not in df.columns]
        if missing:
            st.error(f"âŒ Missing required columns: {', '.join(missing)}")
            st.write("ğŸ“‹ Found columns in CSV:", df.columns.tolist())
            return pd.DataFrame(), pd.Series()

        # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Î±ÏÎ¹Î¸Î¼Î·Ï„Î¹ÎºÏÎ½ Ï€ÎµÎ´Î¯Ï‰Î½
        numeric_cols = [f"A{i}" for i in range(1, 11)] + ["Case_No"]
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')

        # Î‘Ï€ÏŒÏƒÏ€Î±ÏƒÎ· embeddings Î±Ï€ÏŒ Neo4j
        embeddings = []
        valid_ids = []
        with neo4j_service.session() as session:
            for case_no in df["Case_No"]:
                result = session.run("""
                    MATCH (c:Case {id: $id})
                    WHERE c.embedding IS NOT NULL
                    RETURN c.embedding AS embedding
                """, id=int(case_no))
                record = result.single()
                if record and record["embedding"]:
                    embeddings.append(record["embedding"])
                    valid_ids.append(case_no)

        print("ğŸ“‹ Available Case_Nos in CSV:", df["Case_No"].tolist()[:5])
        print("ğŸ“¦ Valid Case_Nos with embeddings:", valid_ids[:5])
        print("ğŸ“Š Total matched embeddings:", len(embeddings))

        # Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± Î¼ÏŒÎ½Î¿ Î³Î¹Î± Î­Î³ÎºÏ…ÏÎµÏ‚ Ï€ÎµÏÎ¹Ï€Ï„ÏÏƒÎµÎ¹Ï‚
        df_filtered = df[df["Case_No"].isin(valid_ids)].copy()

        # Î‘Ï€ÏŒÏƒÏ€Î±ÏƒÎ· ÎµÏ„Î¹ÎºÎµÏ„ÏÎ½
        y = df_filtered["Class_ASD_Traits"].apply(
            lambda x: 1 if str(x).strip().lower() == "yes" else 0
        )

        assert len(embeddings) == len(y), f"âš ï¸ Embeddings: {len(embeddings)}, Labels: {len(y)}"

        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± X
        X = pd.DataFrame(embeddings[:len(y)])

        # Imputation Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ NaN
        if X.isna().any().any():
            st.warning(f"âš ï¸ Found {X.isna().sum().sum()} NaN values in embeddings - applying imputation")
            X = X.fillna(X.mean())

        return X, y

    except Exception as e:
        st.error(f"Data extraction failed: {str(e)}")
        return pd.DataFrame(), pd.Series()

# === Model Evaluation ===
def analyze_embedding_correlations(X: pd.DataFrame, csv_url: str):
    """Î£Ï…ÏƒÏ‡ÎµÏ„Î¯Î¶ÎµÎ¹ ÎºÎ¬Î¸Îµ Î´Î¹Î¬ÏƒÏ„Î±ÏƒÎ· embedding Î¼Îµ Ï„Î± Î±ÏÏ‡Î¹ÎºÎ¬ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ (A1â€“A10, Î´Î·Î¼Î¿Î³ÏÎ±Ï†Î¹ÎºÎ¬)"""
    st.subheader("ğŸ“Œ Featureâ€“Embedding Correlation Analysis")

    try:
        df = pd.read_csv(csv_url, delimiter=";", encoding='utf-8-sig')
        df.columns = [col.strip() for col in df.columns]

        # ÎšÏÎ±Ï„Î¬Î¼Îµ Î¼ÏŒÎ½Î¿ ÏŒÏƒÎ± Case_No Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ ÏƒÏ„Î¿ X
        if "Case_No" not in df.columns:
            st.error("Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ ÏƒÏ„Î®Î»Î· 'Case_No'")
            return

        if len(X) != len(df):
            st.warning("âš ï¸ ÎœÎ®ÎºÎ¿Ï‚ X ÎºÎ±Î¹ CSV Î´ÎµÎ½ Ï„Î±Î¹ÏÎ¹Î¬Î¶Î¿Ï…Î½ â€” Ï€ÏÎ¿ÏƒÏ€Î±Î¸Ï best effort")

        # Î•Ï€Î¹Î»Î¿Î³Î® Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½
        features = [f"A{i}" for i in range(1, 11)] + ["Sex", "Ethnicity", "Jaundice", "Family_mem_with_ASD"]

        df = df[features]
        df = pd.get_dummies(df, drop_first=True)  # Î¼ÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÎºÎ±Ï„Î·Î³Î¿ÏÎ¹ÎºÏÎ½ ÏƒÎµ Î±ÏÎ¹Î¸Î¼Î·Ï„Î¹ÎºÎ¬

        if df.shape[0] != X.shape[0]:
            df = df.iloc[:X.shape[0]]

        corr = pd.DataFrame(index=df.columns, columns=X.columns)

        for feat in df.columns:
            for dim in X.columns:
                corr.at[feat, dim] = np.corrcoef(df[feat], X[dim])[0, 1]

        corr = corr.astype(float)

        fig, ax = plt.subplots(figsize=(12, 6))
        sns.heatmap(corr, cmap="coolwarm", center=0, annot=False)
        ax.set_title("Î£Ï…ÏƒÏ‡Î­Ï„Î¹ÏƒÎ· Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ Î¼Îµ Embedding Î”Î¹Î±ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚")
        st.pyplot(fig)

    except Exception as e:
        st.error(f"âŒ Correlation analysis failed: {str(e)}")
def plot_combined_curves(y_true, y_proba):
    fig, ax = plt.subplots(1, 2, figsize=(12, 5))

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_true, y_proba)
    ax[0].plot(fpr, tpr, label=f"AUC = {roc_auc_score(y_true, y_proba):.3f}")
    ax[0].plot([0, 1], [0, 1], 'k--')
    ax[0].set_xlabel("False Positive Rate")
    ax[0].set_ylabel("True Positive Rate")
    ax[0].set_title("ROC Curve")
    ax[0].legend()

    # Precision-Recall Curve
    precision, recall, _ = precision_recall_curve(y_true, y_proba)
    ax[1].plot(recall, precision, label=f"AP = {average_precision_score(y_true, y_proba):.3f}")
    ax[1].set_xlabel("Recall")
    ax[1].set_ylabel("Precision")
    ax[1].set_title("Precision-Recall Curve")
    ax[1].legend()

    st.pyplot(fig)
def show_permutation_importance(model, X_test, y_test):
    try:
        result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)
        importance_df = pd.DataFrame({
            "Feature": [f"Dim_{i}" for i in range(X_test.shape[1])],
            "Importance": result.importances_mean
        }).sort_values(by="Importance", ascending=False)

        st.subheader("ğŸ“Š Permutation Importance")
        st.bar_chart(importance_df.set_index("Feature").head(15))
    except Exception as e:
        st.warning(f"Could not calculate permutation importance: {str(e)}")   
# === Model Evaluation ===

def evaluate_model(model, X_test, y_test):
    """Comprehensive model evaluation"""
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    # ğŸ“Š ÎšÎ±Ï„Î±Î½Î¿Î¼Î® Ï€Î¹Î¸Î±Î½Î¿Ï„Î®Ï„Ï‰Î½
    st.subheader("ğŸ“‰ ÎšÎ±Ï„Î±Î½Î¿Î¼Î® Ï€Î¹Î¸Î±Î½Î¿Ï„Î®Ï„Ï‰Î½ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·Ï‚")
    fig, ax = plt.subplots()
    ax.hist(y_proba, bins=20, color='skyblue', edgecolor='black')
    ax.set_xlabel("Î Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„Î± ASD Traits")
    ax.set_ylabel("Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î”ÎµÎ¹Î³Î¼Î¬Ï„Ï‰Î½")
    st.pyplot(fig)

    # Check for suspicious performance
    if roc_auc_score(y_test, y_proba) > 0.98:
        st.warning("""
        ğŸš¨ Suspiciously high performance detected. Possible causes:
        1. Data leakage in embeddings
        2. Test set contains training data
        3. Label contamination in graph
        """)

    st.subheader("ğŸ“Š Model Evaluation Metrics")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ROC AUC", f"{roc_auc_score(y_test, y_proba):.3f}")
        st.metric("Precision", f"{precision_score(y_test, y_pred):.3f}")
    with col2:
        st.metric("Recall", f"{recall_score(y_test, y_pred):.3f}")
        st.metric("F1 Score", f"{f1_score(y_test, y_pred):.3f}")

    # Confusion Matrix
    st.subheader("Confusion Matrix")
    cm = confusion_matrix(y_test, y_pred)
    fig, ax = plt.subplots()
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
    ax.set_xlabel('Predicted')
    ax.set_ylabel('Actual')
    st.pyplot(fig)

    # Feature Importance (Gini)
    st.subheader("ğŸ” Feature Importance (Gini)")
    try:
        importances = pd.Series(
            model.named_steps['classifier'].feature_importances_,
            index=[f"Dim_{i}" for i in range(X_test.shape[1])]
        ).sort_values(ascending=False)
        st.bar_chart(importances.head(15))
    except Exception as e:
        st.warning(f"Could not plot feature importance: {str(e)}")

    # Performance Curves
    st.subheader("ğŸ“ˆ Performance Curves")
    plot_combined_curves(y_test, y_proba)

    # Permutation Importance
    show_permutation_importance(model, X_test, y_test)

    # Feature-to-Embedding Correlation Analysis
    csv_url = "https://raw.githubusercontent.com/GiorgosBouh/llm2cypher-asd-app/main/Toddler_Autism_dataset_July_2018_2.csv"
    analyze_embedding_correlations(X_test, csv_url)

# === Model Training ===
@st.cache_resource(show_spinner="Training ASD detection model...")
def train_asd_detection_model() -> Optional[dict]:
    """Trains the ASD detection model with leakage protection"""
    try:
        csv_url = "https://raw.githubusercontent.com/GiorgosBouh/llm2cypher-asd-app/main/Toddler_Autism_dataset_July_2018_2.csv"

        # 1ï¸âƒ£ Î‘Ï†Î±Î¯ÏÎµÏƒÎ· SCREENED_FOR Î³Î¹Î± Ï€ÏÎ¿ÏƒÏ„Î±ÏƒÎ¯Î± Î±Ï€ÏŒ leakage
        remove_screened_for_labels()

        # 2ï¸âƒ£ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÎºÎ±Î¹ ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
        X_raw, y = extract_training_data_from_csv(csv_url)
        X = X_raw.copy()
        X.columns = [f"Dim_{i}" for i in range(X.shape[1])]
        if X.empty or y.empty:
            st.error("âš ï¸ No valid training data available")
            return None

        # 3ï¸âƒ£ Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=Config.TEST_SIZE,
            stratify=y,
            random_state=Config.RANDOM_STATE
        )

        # 4ï¸âƒ£ Pipeline with imputation + SMOTE + classifier
        pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('smote', SMOTE(random_state=Config.RANDOM_STATE)),
            ('classifier', RandomForestClassifier(
                n_estimators=Config.N_ESTIMATORS,
                random_state=Config.RANDOM_STATE,
                class_weight='balanced'
            ))
        ])

        pipeline.fit(X_train, y_train)

        # 5ï¸âƒ£ Î•Ï€Î±Î½Î±Ï„Î¿Ï€Î¿Î¸Î­Ï„Î·ÏƒÎ· labels ÎœÎŸÎÎŸ Î³Î¹Î± Ï„Î¹Ï‚ Ï€ÎµÏÎ¹Ï€Ï„ÏÏƒÎµÎ¹Ï‚ Î¼Îµ embeddings
        reinsert_labels_from_csv(csv_url)

        # 6ï¸âƒ£ Î•Ï€Î¹ÏƒÏ„ÏÎ¿Ï†Î®
        return {
            "model": pipeline,
            "X_test": X_test,
            "y_test": y_test
        }

    except Exception as e:
        st.error(f"âŒ Error training model: {e}")
        logger.error(f"Training error: {e}", exc_info=True)
        return None
# === Anomaly Detection ===
# === Anomaly Detection ===
@safe_neo4j_operation
def get_existing_embeddings() -> Optional[np.ndarray]:
    """Returns all case embeddings for anomaly detection"""
    with neo4j_service.session() as session:
        result = session.run("""
            MATCH (c:Case)
            WHERE c.embedding IS NOT NULL
            RETURN c.embedding AS embedding
        """)
        embeddings = [record["embedding"] for record in result]
        return np.array(embeddings) if embeddings else None

@st.cache_resource(show_spinner="Training Isolation Forest...")
def train_isolation_forest() -> Optional[Tuple[IsolationForest, StandardScaler]]:
    """Trains anomaly detection model"""
    embeddings = get_existing_embeddings()
    if embeddings is None or len(embeddings) < Config.MIN_CASES_FOR_ANOMALY_DETECTION:
        st.warning(f"âš ï¸ Need at least {Config.MIN_CASES_FOR_ANOMALY_DETECTION} cases for anomaly detection")
        return None

    scaler = StandardScaler()
    embeddings_scaled = scaler.fit_transform(embeddings)

    contamination = min(0.1, 5.0 / len(embeddings))  # Dynamic contamination rate
    iso_forest = IsolationForest(
        contamination=contamination,
        random_state=Config.RANDOM_STATE
    )
    iso_forest.fit(embeddings_scaled)

    return iso_forest, scaler
@safe_neo4j_operation
def reinsert_labels_from_csv(csv_url: str):
    """Î•Ï€Î±Î½Î±Ï„Î¿Ï€Î¿Î¸Î­Ï„Î·ÏƒÎ· SCREENED_FOR labels Î±Ï€ÏŒ CSV"""
    df = pd.read_csv(csv_url, delimiter=";", encoding='utf-8-sig')
    df.columns = [col.strip() for col in df.columns]

    if "Case_No" not in df.columns or "Class_ASD_Traits" not in df.columns:
        st.error("âŒ Î¤Î¿ CSV Î´ÎµÎ½ Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ Ï„Î¹Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚ 'Case_No' ÎºÎ±Î¹ 'Class_ASD_Traits'")
        return

    with neo4j_service.session() as session:
        for _, row in df.iterrows():
            case_id = int(row["Case_No"])
            label = str(row["Class_ASD_Traits"]).strip().lower()
            if label in ["yes", "no"]:
                session.run("""
                    MATCH (c:Case {id: $case_id})
                    MERGE (t:ASD_Trait {value: $label})
                    MERGE (c)-[:SCREENED_FOR]->(t)
                """, case_id=case_id, label=label.capitalize())
# === Streamlit UI ===
def main():
    st.title("ğŸ§  NeuroCypher ASD")
    st.markdown("""
        <i>Autism Spectrum Disorder detection using graph embeddings</i>
        """, unsafe_allow_html=True)

    # Sidebar
    st.sidebar.markdown(f"ğŸ”— Connected to: `{os.getenv('NEO4J_URI')}`")

    # Tab system
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ“Š Model Training", 
        "ğŸŒ Graph Embeddings", 
        "ğŸ“¤ Upload New Case", 
        "ğŸ’¬ NLP to Cypher"
    ])

    # === Model Training Tab ===
    with tab1:
        st.header("ğŸ¤– ASD Detection Model")

        if st.button("ğŸ”„ Train/Refresh Model"):
            with st.spinner("Training model with leakage protection..."):
                results = train_asd_detection_model()

                if results:
                    st.session_state.model_results = results
                    st.success("âœ… Model trained successfully!")

                    # âœ… Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
                    evaluate_model(
                        results["model"],
                        results["X_test"],
                        results["y_test"]
                    )

                    # âœ… Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· ÎµÏ€Î±Î½Î±ÏƒÏÎ½Î´ÎµÏƒÎ· ÎµÏ„Î¹ÎºÎµÏ„ÏÎ½
                    with st.spinner("Reattaching labels to cases..."):
                        csv_url = "https://raw.githubusercontent.com/GiorgosBouh/llm2cypher-asd-app/main/Toddler_Autism_dataset_July_2018_2.csv"
                        reinsert_labels_from_csv(csv_url)
                        st.success("ğŸ¯ Labels reinserted automatically after training!")

    # === Graph Embeddings Tab ===
    with tab2:
        st.header("ğŸŒ Graph Embeddings")
        if st.button("ğŸ” Recalculate All Embeddings"):
            st.info("Î‘Ï…Ï„Î® Î· Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î± Î­Ï‡ÎµÎ¹ Î±Ï€ÎµÎ½ÎµÏÎ³Î¿Ï€Î¿Î¹Î·Î¸ÎµÎ¯. Î“Î¹Î± embeddings, Ï„ÏÎ­Î¾Îµ Ï„Î¿ kg_builder_2.py.")

    # === File Upload Tab ===
    with tab3:
        st.header("ğŸ“„ Upload New Case")
        uploaded_file = st.file_uploader(
            "Upload CSV for single case prediction", 
            type="csv",
            key="case_uploader"
        )

        if uploaded_file:
            try:
                df = pd.read_csv(uploaded_file, delimiter=";")
                required_cols = [
                    "Case_No", "A1", "A2", "A3", "A4", "A5", 
                    "A6", "A7", "A8", "A9", "A10",
                    "Sex", "Ethnicity", "Jaundice", 
                    "Family_mem_with_ASD", "Who_completed_the_test"
                ]

                if not all(col in df.columns for col in required_cols):
                    st.error("âŒ Missing required columns in CSV")
                    st.stop()

                if len(df) != 1:
                    st.error("Please upload exactly one case")
                    st.stop()

                # Process file
                upload_id = str(uuid.uuid4())
                row = df.iloc[0]

                # 1. Insert case
                with st.spinner("Inserting case into graph..."):
                    upload_id = insert_user_case(row, upload_id)
                    st.session_state.last_upload_id = upload_id

                # 2. Generate embedding just for this case
                with st.spinner("Generating graph embedding for new case..."):
                    success = generate_embedding_for_case(neo4j_service.driver, upload_id)
                    if not success:
                        st.error("âŒ Failed to generate embedding. The case may be too isolated in the graph.")
                        st.stop()           

                # 3. Extract embedding
                with st.spinner("Extracting case embedding..."):
                    embedding = extract_user_embedding(upload_id)
                    if embedding is None:
                        st.stop()
                    st.session_state.current_embedding = embedding

                st.subheader("ğŸ§  Case Embedding")
                st.write(embedding)

                # === DEBUGGING INFO for New Case ===
                st.subheader("ğŸ§ª Embedding Diagnostics")

                st.text("ğŸ“¦ Embedding vector:")
                st.write(embedding.tolist())

                st.text("âœ… Embedding integrity check:")
                if embedding is None:
                    st.error("âŒ Embedding is None.")
                elif np.isnan(embedding).any():
                    st.error("âŒ Embedding contains NaN values.")
                else:
                    st.success("âœ… Embedding is valid (no NaNs)")

                with neo4j_service.session() as session:
                    degree_result = session.run("""
                        MATCH (c:Case {upload_id: $upload_id})--(n)
                        RETURN count(n) AS degree
                    """, upload_id=upload_id)
                    degree = degree_result.single()["degree"]
                    st.text(f"ğŸ”— Number of connected nodes: {degree}")
                    if degree < 5:
                        st.warning("âš ï¸ Very few connections in the graph. The embedding might be weak.")

                if "model_results" in st.session_state:
                    X_train = st.session_state.model_results["X_test"]
                    train_mean = X_train.mean().values
                    dist = np.linalg.norm(embedding - train_mean)
                    st.text(f"ğŸ“ Distance from train mean: {dist:.4f}")
                    if dist > 5.0:
                        st.warning("âš ï¸ Embedding far from training distribution. Prediction may be unreliable.")

                # 4. Make prediction if model exists
                if "model_results" in st.session_state:
                    model = st.session_state.model_results["model"]
                    proba = model.predict_proba(embedding)[0][1]
                    prediction = "ASD Traits Detected" if proba >= 0.5 else "Typical Development"

                    st.subheader("ğŸ” Prediction Result")
                    col1, col2 = st.columns(2)
                    col1.metric("Prediction", prediction)
                    col2.metric("Confidence", f"{max(proba, 1-proba):.1%}")

                    fig = px.bar(
                        x=["Typical", "ASD Traits"],
                        y=[1-proba, proba],
                        title="Prediction Probabilities"
                    )
                    st.plotly_chart(fig)

                # 5. Anomaly detection
                with st.spinner("Running anomaly detection..."):
                    iso_result = train_isolation_forest()
                    if iso_result:
                        iso_forest, scaler = iso_result
                        embedding_scaled = scaler.transform(embedding)
                        anomaly_score = iso_forest.decision_function(embedding_scaled)[0]
                        is_anomaly = iso_forest.predict(embedding_scaled)[0] == -1

                        st.subheader("ğŸ•µï¸ Anomaly Detection")
                        if is_anomaly:
                            st.warning(f"âš ï¸ Anomaly detected (score: {anomaly_score:.3f})")
                        else:
                            st.success(f"âœ… Normal case (score: {anomaly_score:.3f})")

            except Exception as e:
                st.error(f"âŒ Error processing file: {str(e)}")
                logger.error(f"Upload error: {str(e)}", exc_info=True)

    # === NLP to Cypher Tab ===
    with tab4:
        st.header("ğŸ’¬ Natural Language to Cypher")
        question = st.text_input("Ask about the data:")

        if question:
            cypher = nl_to_cypher(question)
            if cypher:
                st.code(cypher, language="cypher")

                if st.button("â–¶ï¸ Execute Query"):
                    with neo4j_service.session() as session:
                        try:
                            results = session.run(cypher).data()
                            if results:
                                st.dataframe(pd.DataFrame(results))
                            else:
                                st.info("No results found")
                        except Exception as e:
                            st.error(f"Query failed: {str(e)}")

# === App entrypoint ===
if __name__ == "__main__":
    main()
