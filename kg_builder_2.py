print("=== RUNNING kg_builder_2.py from Git repo ===", flush=True)
import pandas as pd
import numpy as np
from neo4j import GraphDatabase
import networkx as nx
from node2vec import Node2Vec
from random import shuffle
import traceback
import sys
from random import randint
import os
import shutil  # Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Ï„Î·Ï‚ Î»ÎµÎ¯Ï€ÎµÎ¹Ï‚ Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎ·Ï‚


def connect_to_neo4j(uri="neo4j+s://1f5f8a14.databases.neo4j.io", user="neo4j", password="3xhy4XKQSsSLIT7NI-w9m4Z7Y_WcVnL1hDQkWTMIoMQ"):
    print(f"ğŸŒ Connecting to Neo4j Aura: {uri}", flush=True)
    return GraphDatabase.driver(uri, auth=(user, password))

def parse_csv(file_path):
    df = pd.read_csv(file_path, sep=";", encoding="utf-8-sig")
    df.columns = [col.strip() for col in df.columns]
    df = df.apply(lambda col: col.str.strip() if col.dtypes == 'object' else col)

    numeric_cols = ['Case_No', 'A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons', 'Qchat-10-Score']
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col].astype(str).str.replace(",", "."), errors='coerce')

    print("âœ… ÎšÎ±Î¸Î±ÏÎ¹ÏƒÏ„Î·ÎºÎ±Î½ Î¿Î¹ ÏƒÏ„Î®Î»ÎµÏ‚:", df.columns.tolist(), flush=True)
    return df.dropna()

def create_nodes(tx, df):
    for q in [f"A{i}" for i in range(1, 11)]:
        tx.run("MERGE (:BehaviorQuestion {name: $q})", q=q)

    for column in ["Sex", "Ethnicity", "Jaundice", "Family_mem_with_ASD"]:
        for val in df[column].dropna().unique():
            tx.run("MERGE (:DemographicAttribute {type: $type, value: $val})", type=column, val=val)

    for val in df["Who_completed_the_test"].dropna().unique():
        tx.run("MERGE (:SubmitterType {type: $val})", val=val)

def create_relationships(tx, df):
    case_data = []
    answer_data, demo_data, submitter_data = [], [], []

    for _, row in df.iterrows():
        case_id = int(row["Case_No"])
        upload_id = str(case_id)
        case_data.append({"id": case_id, "upload_id": upload_id})

        for q in [f"A{i}" for i in range(1, 11)]:
            answer_data.append({"upload_id": upload_id, "q": q, "val": int(row[q])})

        for col in ["Sex", "Ethnicity", "Jaundice", "Family_mem_with_ASD"]:
            demo_data.append({"upload_id": upload_id, "type": col, "val": row[col]})

        submitter_data.append({"upload_id": upload_id, "val": row["Who_completed_the_test"]})

    tx.run("""
        UNWIND $data as row 
        MERGE (c:Case {id: row.id}) 
        SET c.upload_id = row.upload_id, c.embedding = null
    """, data=case_data)

    tx.run("""
        UNWIND $data as row
        MATCH (q:BehaviorQuestion {name: row.q})
        MATCH (c:Case {upload_id: row.upload_id})
        MERGE (c)-[:HAS_ANSWER {value: row.val}]->(q)
    """, data=answer_data)

    tx.run("""
        UNWIND $data as row
        MATCH (d:DemographicAttribute {type: row.type, value: row.val})
        MATCH (c:Case {upload_id: row.upload_id})
        MERGE (c)-[:HAS_DEMOGRAPHIC]->(d)
    """, data=demo_data)

    tx.run("""
        UNWIND $data as row
        MATCH (s:SubmitterType {type: row.val})
        MATCH (c:Case {upload_id: row.upload_id})
        MERGE (c)-[:SUBMITTED_BY]->(s)
    """, data=submitter_data)

def create_similarity_relationships(tx, df, max_pairs=10000):
    pairs = set()
    
    # 1. Î£Ï…Î¼Ï€ÎµÏÎ¹Ï†Î¿ÏÎ¹ÎºÎ® Î¿Î¼Î¿Î¹ÏŒÏ„Î·Ï„Î± (A1-A10)
    for i, row1 in df.iterrows():
        for j, row2 in df.iloc[i+1:].iterrows():
            if sum(row1[f'A{k}'] == row2[f'A{k}'] for k in range(1,11)) >= 7:
                pairs.add((int(row1['Case_No']), int(row2['Case_No'])))

    # 2. Î”Î·Î¼Î¿Î³ÏÎ±Ï†Î¹ÎºÎ® Î¿Î¼Î¿Î¹ÏŒÏ„Î·Ï„Î±
    demo_cols = ['Sex', 'Ethnicity', 'Jaundice', 'Family_mem_with_ASD']
    for col in demo_cols:
        grouped = df.groupby(col)['Case_No'].apply(list)
        for case_list in grouped:
            for i in range(len(case_list)):
                for j in range(i+1, len(case_list)):
                    pairs.add((int(case_list[i]), int(case_list[j])))

    # Î•Ï†Î±ÏÎ¼Î¿Î³Î® Î¿ÏÎ¯Î¿Ï… ÎºÎ±Î¹ Ï„Ï…Ï‡Î±Î¹Î¿Ï€Î¿Î¯Î·ÏƒÎ·
    pair_list = list(pairs)[:max_pairs]
    shuffle(pair_list)

    tx.run("""
        UNWIND $batch AS pair
        MATCH (c1:Case {id: pair.id1}), (c2:Case {id: pair.id2})
        MERGE (c1)-[:SIMILAR_TO]->(c2)
    """, batch=[{'id1':x, 'id2':y} for x,y in pair_list])

def generate_embeddings(driver):
    temp_folder_path = os.path.join(os.getcwd(), 'node2vec_temp')
    os.makedirs(temp_folder_path, exist_ok=True)
    G = nx.Graph()
    
    print("â³ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î³ÏÎ¬Ï†Î¿Ï… Î±Ï€ÏŒ Neo4j...", flush=True)
    
    with driver.session() as session:
        # Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Case ÎºÏŒÎ¼Î²Ï‰Î½
        cases = session.run("MATCH (c:Case) RETURN c.id AS id")
        case_ids = [f"Case_{record['id']}" for record in cases]
        G.add_nodes_from(case_ids, type="Case")
        print(f"ğŸ“Š Î¦Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ {len(case_ids)} ÎºÏŒÎ¼Î²Î¿Î¹ Case", flush=True)
        
        # Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿ query Î³Î¹Î± Ï†ÏŒÏÏ„Ï‰ÏƒÎ· ÏƒÏ‡Î­ÏƒÎµÏ‰Î½
        relationships = session.run("""
            MATCH (c:Case)-[r]->(n)
            RETURN c.id AS source_id, 
                   type(r) AS rel_type,
                   CASE 
                     WHEN n:BehaviorQuestion THEN 'Q_' + n.name
                     WHEN n:DemographicAttribute THEN 'D_' + n.type + '_' + replace(n.value, ' ', '_')
                     WHEN n:SubmitterType THEN 'S_' + replace(n.type, ' ', '_')
                     WHEN n:Case THEN 'Case_' + toString(n.id)
                     ELSE 'Node_' + toString(id(n))
                   END AS target_id
        """)
        
        edge_count = 0
        for record in relationships:
            source = f"Case_{record['source_id']}"
            target = record['target_id']
            G.add_node(target)
            G.add_edge(source, target, type=record['rel_type'])
            edge_count += 1
        
        print(f"ğŸ“Š Î¦Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ {edge_count} Î±ÎºÎ¼Î­Ï‚", flush=True)
    
    print(f"ğŸ“ˆ Î¤ÎµÎ»Î¹ÎºÏŒÏ‚ Î³ÏÎ¬Ï†Î¿Ï‚: {len(G.nodes)} ÎºÏŒÎ¼Î²Î¿Î¹, {len(G.edges)} Î±ÎºÎ¼Î­Ï‚", flush=True)
    
    if len(G.edges) == 0:
        raise ValueError("âŒ ÎŸ Î³ÏÎ¬Ï†Î¿Ï‚ ÎµÎ¯Î½Î±Î¹ Î¬Î´ÎµÎ¹Î¿Ï‚! ÎˆÎ»ÎµÎ³Î¾Îµ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± ÏƒÏ„Î· Neo4j.")
    
    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± embeddings Î¼ÏŒÎ½Î¿ Î³Î¹Î± ÎºÏŒÎ¼Î²Î¿Ï…Ï‚ Case
    case_nodes = [n for n in G.nodes if G.nodes[n].get('type') == 'Case']
    print(f"ğŸ” Î˜Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î·Î¸Î¿ÏÎ½ embeddings Î³Î¹Î± {len(case_nodes)} ÎºÏŒÎ¼Î²Î¿Ï…Ï‚ Case", flush=True)
    
    # Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Node2Vec Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î±Ï€ÏŒÎ´Î¿ÏƒÎ·
    node2vec = Node2Vec(
        G,
        dimensions=128,
        walk_length=30,
        num_walks=100,  # ÎœÎµÎ¹Ï‰Î¼Î­Î½Î¿ Î³Î¹Î± Ï„Î±Ï‡ÏÏ„ÎµÏÎ· ÎµÎºÏ„Î­Î»ÎµÏƒÎ·
        workers=4,
        p=1.0,
        q=0.5,
        temp_folder=temp_folder_path,
        quiet=True  # Î›Î¹Î³ÏŒÏ„ÎµÏÎ¿ output
    )
    
    try:
        model = node2vec.fit(
            window=10,
            min_count=1,
            batch_words=128
        )
        
        # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· embeddings Î¼Îµ batch
        with driver.session() as session:
            batch_size = 500
            batch = []
            saved_count = 0
            
            for node in case_nodes:
                case_id = int(node.split('_')[1])
                try:
                    embedding = model.wv[node].tolist()
                    batch.append({"case_id": case_id, "embedding": embedding})
                    
                    if len(batch) >= batch_size:
                        session.run("""
                            UNWIND $batch AS item
                            MATCH (c:Case {id: item.case_id})
                            SET c.embedding = item.embedding
                        """, {"batch": batch})
                        saved_count += len(batch)
                        batch = []
                        print(f"âœ… Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎ±Î½ {saved_count}/{len(case_nodes)} embeddings", flush=True)
                
                except KeyError:
                    print(f"âš ï¸ Î§Ï‰ÏÎ¯Ï‚ embedding: {node}", flush=True)
                    continue
            
            if batch:
                session.run("""
                    UNWIND $batch AS item
                    MATCH (c:Case {id: item.case_id})
                    SET c.embedding = item.embedding
                """, {"batch": batch})
                saved_count += len(batch)
        
        print(f"âœ… ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ Î· Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· {saved_count} embeddings", flush=True)
        return True
    
    except Exception as e:
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î±: {str(e)}", flush=True)
        traceback.print_exc()
        return False
    finally:
        # ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï€ÏÎ¿ÏƒÏ‰ÏÎ¹Î½Î¿Ï Ï†Î±ÎºÎ­Î»Î¿Ï… Î¼Îµ Î­Î»ÎµÎ³Ï‡Î¿ ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½
        try:
            if os.path.exists(temp_folder_path):
                shutil.rmtree(temp_folder_path)
        except Exception as e:
            print(f"âš ï¸ Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼Î¿Ï temp folder: {str(e)}", flush=True)

def build_graph():
    driver = connect_to_neo4j()
    file_path = "https://raw.githubusercontent.com/GiorgosBouh/llm2cypher-asd-app/main/Toddler_Autism_dataset_July_2018_2.csv"

    try:
        print("â³ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÎºÎ±Î¹ ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½...", flush=True)
        df = parse_csv(file_path)
        print("ğŸ§  Î”ÎµÎ¯Î³Î¼Î± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½:", df.iloc[0].to_dict(), flush=True)

        with driver.session() as session:
            print("ğŸ§¹ Î”Î¹Î±Î³ÏÎ±Ï†Î® Ï€Î±Î»Î¹Î¿Ï Î³ÏÎ¬Ï†Î¿Ï…...", flush=True)
            session.run("MATCH (n) DETACH DELETE n")

            print("â³ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î½Î­Ï‰Î½ ÎºÏŒÎ¼Î²Ï‰Î½...", flush=True)
            session.execute_write(create_nodes, df)

            print("â³ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î²Î±ÏƒÎ¹ÎºÏÎ½ ÏƒÏ‡Î­ÏƒÎµÏ‰Î½...", flush=True)
            session.execute_write(create_relationships, df)

            print("â³ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÏƒÏ‡Î­ÏƒÎµÏ‰Î½ Î¿Î¼Î¿Î¹ÏŒÏ„Î·Ï„Î±Ï‚...", flush=True)
            session.execute_write(create_similarity_relationships, df)

        print("â³ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± graph embeddings...", flush=True)
        if not generate_embeddings(driver):
            raise RuntimeError("Failed to generate embeddings")

        print("âœ… ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚ Î· Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î³ÏÎ¬Ï†Î¿Ï…!", flush=True)
        sys.exit(0)

    except Exception as e:
        print(f"âŒ ÎšÏÎ¯ÏƒÎ¹Î¼Î¿ ÏƒÏ†Î¬Î»Î¼Î±: {str(e)}", flush=True)
        traceback.print_exc()
        sys.exit(1)

    finally:
        driver.close()

if __name__ == "__main__":
    build_graph()